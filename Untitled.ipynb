{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTHORSHIP ATTRIBUTION\n",
    "\n",
    "<!-- vim-markdown-toc -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We introduce a syntactic recurrent neural network to encode the syn-\n",
    "tactic patterns of a document in a hierarchical structure. First, we\n",
    "represent each sentence as a sequence of POS tags and each POS tag\n",
    "is embedded into a low dimensional vector and a POS encoder (which\n",
    "can be a CNN or LSTM) learns the syntactic representation of sen-\n",
    "tences. Subsequently, the learned sentence representations aggregate\n",
    "into the document representation. Moreover, we use attention mecha-\n",
    "nism to reward the sentences which contribute more to the prediction\n",
    "of labels. Afterwards we use a soft-max classifier to compute the prob-ability distribution over class labels.\n",
    "The overall architecture of the network is\n",
    "\n",
    "<img src=\"images/smai1.png\" ></img>\n",
    "\n",
    "## Dataset :\n",
    "We used Pan 2012 dataset URL : https://pan.webis.de/data.html . The Given DataSet (Novels and their Writers)is of the form of text\n",
    "files.\n",
    "The name of these Text files are as follows-\n",
    "12ItrainA1.TXT,\n",
    "12ItrainB2.TXT,\n",
    "12ItrainD2.TXT,\n",
    "12ItrainF2.TXT,\n",
    "12ItrainH2.TXT,\n",
    "12ItrainJ2.TXT,\n",
    "12ItrainL1.TXT,\n",
    "12ItrainM2.TXT,\n",
    "12ItrainN3.TXT,\n",
    "12ItrainA2.TXT,\n",
    "12ItrainC1.TXT,\n",
    "12ItrainE1.TXT,\n",
    "12ItrainG1.TXT,\n",
    "12ItrainI1.TXT,\n",
    "12ItrainK1.TXT,\n",
    "12ItrainL2.TXT,\n",
    "12ItrainM3.TXT,\n",
    "12ItrainA3.TXT,\n",
    "12ItrainB1.TXT,\n",
    "12ItrainC2.TXT,\n",
    "12ItrainD1.TXT,\n",
    "12ItrainE2.TXT,\n",
    "12ItrainF1.TXT,\n",
    "12ItrainG2.TXT,\n",
    "12ItrainH1.TXT,\n",
    "12ItrainI2.TXT,\n",
    "12ItrainJ1.TXT,\n",
    "12ItrainK2.TXT,\n",
    "12ItrainK3.TXT,\n",
    "12ItrainL3.TXT,\n",
    "12ItrainM1.TXT,\n",
    "12ItrainN1.TXT,\n",
    "12ItrainN2.TXT\n",
    "\n",
    "Such that the name of the Authors are after the keyword ’train’ and\n",
    "then after the name of the author the number which is to differentiate\n",
    "between different novels of the same author.\n",
    "\n",
    "<img src=\"images/9.png\" ></img>\n",
    "\n",
    "## POS Embedding\n",
    "We assume that each document is a sequence of M sentences and\n",
    "each sentence is a sequence of N words, where M , and N are model\n",
    "hyper parameters.Given a sentence, we convert each word into the\n",
    "corresponding POS tag in the sentence and afterwards we embed each\n",
    "POS tag into a low dimensional vector P,using a trainable lookup\n",
    "table.\n",
    "\n",
    "### Loading Novels and Removing Stop words \n",
    "The model first takes the Dataset, Reads it and converts all the words\n",
    "to token.\n",
    "As shown below-\n",
    "<img src=\"images/1.png\" ></img>\n",
    "\n",
    "There are certain words which are useless as tokens to aur models and\n",
    "are very frequent in the novels,they do not make any difference and\n",
    "does not even tell the style of the author.Hence we need to remove\n",
    "them.\n",
    "<img src=\"images/2.png\" ></img>\n",
    "\n",
    "### Words to POS tags and then to sequence number\n",
    "For further processing of Data , we need to change every word to\n",
    "part-of-speech tag.We use NLTK part-of-speech tagger for the tagging\n",
    "purpose and use the set of 47 POS tags in our model as follows.\n",
    "<img src=\"images/smai2.png\" ></img>\n",
    "<img src=\"images/4.png\" ></img>\n",
    "\n",
    "The POS tags has a numbers assigned to it. The tags generated are\n",
    "converted to sequence number using this mapping.\n",
    "<img src=\"images/6.png\" ></img>\n",
    "\n",
    "Then, padding is added to each sentence to make them of the same\n",
    "length.\n",
    "<img src=\"images/7.png\" ></img>\n",
    "\n",
    "### POS encoder \n",
    "POS encoder learns the syntactic representation of sentences from the\n",
    "output of POS embedding layer. In order to investigate the effect of\n",
    "short-term and long-term dependencies of POS tags in the sentence,\n",
    "we exploit both CNNs and LSTMs.\n",
    "####  Short-term Dependencies\n",
    "CNNs generally capture the short-term dependencies of words in the\n",
    "sentences which make them robust to the varying length of sentences\n",
    "in the documents\n",
    "####  Long-term Dependencies\n",
    "Recurrent neural networks especially LSTMs are capable of capturing\n",
    "the long-term relations in se- quences which make them more effec-\n",
    "tive compared to the conventional n-gram models where increasing\n",
    "the length of sequences results a sparse matrix representation of doc-\n",
    "uments.\n",
    "\n",
    "### Sentence encoder \n",
    "Sentence encoder learns the syntactic representation of a document\n",
    "from the sequence of sentence representations outputted from the POS\n",
    "encoder. We use a bidirectional LSTM To capture how sentences with\n",
    "different syntactic patterns are structured in a document.\n",
    "\n",
    "### Classification \n",
    "The learned vector representation of documents are fed into a soft-\n",
    "max classifier to compute the probability distribution of class labels.\n",
    "The model parameters are optimized to minimize the cross-entropy loss over all the documents in the training corpus.\n",
    "\n",
    "## Experimental Results\n",
    "We report both segment-level and document-level accuracy. As men-\n",
    "tioned before, each document (novel) has been divided into the seg-\n",
    "ments of 100 sentences. Therefore, each segment in a novel has classi-\n",
    "fied independently and afterwards the label of each document is cal-\n",
    "culated as the majority voting of its constituent segments.\n",
    "\n",
    "Document level Accuracy for both LSTM-LSTM and CNN-\n",
    "LSTM Model was 100% (14/14 novels)\n",
    "\n",
    "#### Graph for Training and Validation Accuracy for LSTM-LSTM\n",
    "<img src=\"images/lstm_lstm.png\" ></img>\n",
    "The Validation Accuracy for LSTM-LSTM Model achieved\n",
    "was 72%\n",
    "#### Graph for Training and Validation Accuracy for CNN-LSTM\n",
    "<img src=\"images/cnnlstm.png\" ></img>\n",
    "The Validation Accuracy for CNN-LSTM Model achieved\n",
    "was 71%\n",
    "\n",
    "## Hindi-Dataset Results\n",
    "Apart from English novels and their writers, we also experimented us-\n",
    "ing Hindi novel.\n",
    "First, we Downloaded Hindi novels in the form of text files. Convert-\n",
    "ing them to POS tags by using NLTK part-of-speech tagger for the\n",
    "tagging purpose of type ’INDIAN’ and then after this passing the tags\n",
    "through the LSTM-LSTM model the same way as english data set.\n",
    "There is an abnormal behavior of Model for HIndi dataset,\n",
    "As for the POS Tagging we used NLTK standard library but\n",
    "for hindi one this is incomplete .Majority of times it assigns\n",
    "UNK i.e., UNKNOWN POS TAG for the word.Hence finds\n",
    "difficult to learn.\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "* The above work is contributed by myself, Sivangi Singh, Monu Tayal and Girdhari LAl Gupta .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
