{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaking whole data into list of sentences\n",
    "'''\n",
    "def seperate_sentences(data):\n",
    "    return sent_tokenize(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_to_POS(all_sentences):\n",
    "\n",
    "    data=[]\n",
    "    for sentence in all_sentences: \n",
    "        one_sentence_pos = []\n",
    "        '''\n",
    "        Word tokenizers is used to find the words and punctuation in a string \n",
    "        '''  \n",
    "        wordsList = nltk.word_tokenize(sentence) \n",
    "\n",
    "        '''\n",
    "        Removing stop words from wordList.\n",
    "        ''' \n",
    "        wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "\n",
    "        '''\n",
    "        Using a Tagger. Which is part-of-speech tagger or POS-tagger.  \n",
    "        '''\n",
    "\n",
    "        tagged = nltk.pos_tag(wordsList) \n",
    "        for x in tagged:\n",
    "            one_sentence_pos.append(x[1])\n",
    "        data.append(one_sentence_pos)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_More_Data(novel_POS,novel_label,number_of_sentenes_in_one):\n",
    "    '''\n",
    "    This funtion takes generates more data for training by breaking one novel into multiple novels\n",
    "    with k number of sentences (number_of_sentenes_in_one) .we are ignoring some last sentences if they\n",
    "    dont fit\n",
    "    '''\n",
    "    i=0\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for novel in novel_POS:\n",
    "        start = 0\n",
    "        end = number_of_sentenes_in_one\n",
    "        while(end<=len(novel)):\n",
    "            X.append(novel[start:start+end])\n",
    "            Y.append(novel_label[i])\n",
    "            start=end\n",
    "            end=end+number_of_sentenes_in_one\n",
    "        i+=1\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_X_and_Y_In_POS_Form(path_of_dataset_directory,number_of_sentences_in_one):\n",
    "    '''\n",
    "    this function takes the path of directory where the dataset is and returns the processed X and Y.\n",
    "    The function calls will let u understand more the flow of code\n",
    "    '''\n",
    "    all_files = os.listdir(path_of_dataset_directory)\n",
    "    novel_POS=[]\n",
    "    novel_label=[]\n",
    "    for filename in all_files:\n",
    "        full_path = 'dataset/three_author_dataset/'+filename\n",
    "        with open(full_path, 'r') as f:\n",
    "            data = f.read().replace('\"\\n\"','').replace('\\n',' ').replace('- ','')\n",
    "            all_sentences = seperate_sentences(data)\n",
    "            sentences_to_pos = Sentence_to_POS(all_sentences)\n",
    "            novel_POS.append(sentences_to_pos)\n",
    "            novel_label.append(filename[8])\n",
    "    return Fianl_X_and_Y(novel_POS,novel_label,number_of_sentences_in_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_index_dictionary(X):\n",
    "    '''\n",
    "    This function just creates a dictionary from the POS tags which we\n",
    "    encountered in our dataset.As the network works with numbers so simple will make\n",
    "    dictionary which stores the index of associated POS tag\n",
    "    '''\n",
    "    tag = set([])\n",
    "    for doc in X:\n",
    "        for sentence in doc:\n",
    "            for word in sentence:\n",
    "                tag.add(word)\n",
    "    tag2index = {t: i + 1 for i, t in enumerate(list(tag))}\n",
    "    tag2index['-PAD-'] = 0\n",
    "    return tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag_to_sequence_numbers(X):\n",
    "    tag2index = tag_to_index_dictionary(X)\n",
    "    '''\n",
    "    using the tag2index dictionary assign indexs to the POS tags in our data\n",
    "    '''\n",
    "    new_X=[]\n",
    "    for doc in X:\n",
    "        new_S = []\n",
    "        for sentence in doc:\n",
    "            new_W=[]\n",
    "            for word in sentence:\n",
    "                new_W.append(tag2index[word])\n",
    "            new_S.append(new_W)\n",
    "        new_X.append(new_S)\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros_to_sequence(X,max_length):\n",
    "    '''\n",
    "    This function padds zeros to the sequences so as to make fixed sequences\n",
    "    According to paper 15 is best value for max_length\n",
    "    '''\n",
    "    \n",
    "    new_X=[]\n",
    "    for doc in X:\n",
    "        new_X.append(pad_sequences(doc, maxlen=max_length, padding='post'))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encode_Labels(Y):\n",
    "    '''\n",
    "    This fucntion encodes the labels i.e., assign numbers to the labels\n",
    "    '''\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = Get_X_and_Y_In_POS_Form('dataset/three_author_dataset',80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_tag_to_sequence_numbers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_zeros_to_sequence(X,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Encode_Labels(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
