{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "stop_words = set(stopwords.words('english')) \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout, Activation,Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaking whole data into list of sentences\n",
    "'''\n",
    "def seperate_sentences(data):\n",
    "    return sent_tokenize(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_to_POS(all_sentences):\n",
    "\n",
    "    data=[]\n",
    "    for sentence in all_sentences: \n",
    "        one_sentence_pos = []\n",
    "        '''\n",
    "        Word tokenizers is used to find the words and punctuation in a string \n",
    "        '''  \n",
    "        wordsList = nltk.word_tokenize(sentence) \n",
    "\n",
    "        '''\n",
    "        Removing stop words from wordList.\n",
    "        ''' \n",
    "        wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "\n",
    "        '''\n",
    "        Using a Tagger. Which is part-of-speech tagger or POS-tagger.  \n",
    "        '''\n",
    "\n",
    "        tagged = nltk.pos_tag(wordsList) \n",
    "        for x in tagged:\n",
    "            one_sentence_pos.append(x[1])\n",
    "        data.append(one_sentence_pos)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fianl_X_and_Y(novel_POS,novel_label,number_of_sentenes_in_one):\n",
    "    '''\n",
    "    This funtion takes generates more data for training by breaking one novel into multiple novels\n",
    "    with k number of sentences (number_of_sentenes_in_one) .we are ignoring some last sentences if they\n",
    "    dont fit\n",
    "    '''\n",
    "    i=0\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for novel in novel_POS:\n",
    "        start = 0\n",
    "        end = number_of_sentenes_in_one\n",
    "        while(end<=len(novel)):\n",
    "            X.append(novel[start:end])\n",
    "            Y.append(novel_label[i])\n",
    "            start=end\n",
    "            end=end+number_of_sentenes_in_one\n",
    "        i+=1\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_X_and_Y_In_POS_Form(path_of_dataset_directory,number_of_sentences_in_one):\n",
    "    '''\n",
    "    this function takes the path of directory where the dataset is and returns the processed X and Y.\n",
    "    The function calls will let u understand more the flow of code\n",
    "    '''\n",
    "    all_files = os.listdir(path_of_dataset_directory)\n",
    "    novel_POS=[]\n",
    "    novel_label=[]\n",
    "    for filename in all_files:\n",
    "        full_path = 'dataset/three_author_dataset/'+filename\n",
    "        with open(full_path, 'r') as f:\n",
    "            data = f.read().replace('\"\\n\"','').replace('\\n',' ').replace('- ','')\n",
    "            all_sentences = seperate_sentences(data)\n",
    "            sentences_to_pos = Sentence_to_POS(all_sentences)\n",
    "            novel_POS.append(sentences_to_pos)\n",
    "            novel_label.append(filename[8])\n",
    "    return Fianl_X_and_Y(novel_POS,novel_label,number_of_sentences_in_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_index_dictionary(X):\n",
    "    '''\n",
    "    This function just creates a dictionary from the POS tags which we\n",
    "    encountered in our dataset.As the network works with numbers so simple will make\n",
    "    dictionary which stores the index of associated POS tag\n",
    "    '''\n",
    "    tag = set([])\n",
    "    for doc in X:\n",
    "        for sentence in doc:\n",
    "            for word in sentence:\n",
    "                tag.add(word)\n",
    "    tag2index = {t: i + 1 for i, t in enumerate(list(tag))}\n",
    "    tag2index['-PAD-'] = 0\n",
    "    return tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag_to_sequence_numbers(X):\n",
    "    tag2index = tag_to_index_dictionary(X)\n",
    "    '''\n",
    "    using the tag2index dictionary assign indexs to the POS tags in our data\n",
    "    '''\n",
    "    new_X=[]\n",
    "    for doc in X:\n",
    "        new_S = []\n",
    "        for sentence in doc:\n",
    "            new_W=[]\n",
    "            for word in sentence:\n",
    "                new_W.append(tag2index[word])\n",
    "            new_S.append(new_W)\n",
    "        new_X.append(new_S)\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros_to_sequence(X,max_length):\n",
    "    '''\n",
    "    This function padds zeros to the sequences so as to make fixed sequences\n",
    "    According to paper 15 is best value for max_length\n",
    "    '''\n",
    "    \n",
    "    new_X=[]\n",
    "    for doc in X:\n",
    "        new_X.append(pad_sequences(doc, maxlen=max_length, padding='post'))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encode_Labels(Y):\n",
    "    '''\n",
    "    This fucntion encodes the labels i.e., assign numbers to the labels\n",
    "    '''\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = Get_X_and_Y_In_POS_Form('dataset/three_author_dataset',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "        'N'], dtype='<U1'),\n",
       " array([ 95, 264, 191, 125, 180,  72, 100, 159,  89, 104,  79,  47,  60,\n",
       "         74]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_LENGTH = len(tag_to_index_dictionary(X))\n",
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_tag_to_sequence_numbers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_zeros_to_sequence(X,MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Encode_Labels(Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1639, 100, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X_train = X.reshape(1639,100*30)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1639, 14)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_binary = to_categorical(Y)\n",
    "y_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1311 samples, validate on 328 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(DICTIONARY_LENGTH, 100, input_length=3000))\n",
    "model.add(LSTM(100, recurrent_dropout=0.2,return_sequences=True,activation='relu'))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(14, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_binary, validation_split=0.2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 80, 15)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
